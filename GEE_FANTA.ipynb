{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Reformat this notebook. Move (most) of the explanation comments to their own markup cells.\n",
    "\n",
    "#imports, global consts, inits\n",
    "#The try-catch block bellow to avoid initialising stuff every run. Sometimes I run this entire workbook when changing input, but I don't need GEE/geemap to be\n",
    "#reinitialised. This saves me a few seconds (a drop in the ocean, compared to how long computation of large rois take, but still...)\n",
    "try:\n",
    "    isInitialised\n",
    "\n",
    "except: #Will always trigger if this is a new kernel (i.e. you restarted Jupyter).\n",
    "    print (\"Initialising Earth Engine API\")\n",
    "    isInitialised = True\n",
    "\n",
    "    import ee\n",
    "    import geemap\n",
    "\n",
    "    #NOTE: If you haven't set a default project for earthengine to use via CLI, you'll need to provide ee.Initialize() with project name\n",
    "    #see https://developers.google.com/earth-engine/guides/auth\n",
    "    ee.Initialize()\n",
    "    map = geemap.Map()\n",
    "\n",
    "    sudanStateBorders = ee.FeatureCollection(\"projects/seamproj01/assets/SudanStateBorders\")    #Shapefiles for Sudan administration borders, via OCHA HDX. This dataset is used for \n",
    "                                                                                                #state/admin division clipping (offline, on QGIS) of the cropland masks bellow. Mostly used\n",
    "                                                                                                #when testing with geemap.\n",
    "\n",
    "    testArea =  ee.FeatureCollection(\"projects/seamproj01/assets/test_area_v2\") #A small block in the Gezira state east of the Blue Nile, a subset of geziraCropland\n",
    "\n",
    "    khartoumCropland = ee.FeatureCollection(\"projects/seamproj01/assets/khartoum_cropmask_v4_1\")    #From the Copernicus Moderate Dynamic Land Cover dataset. Extracted cropland pixels, then clipped\n",
    "                                                                                                    #to Khartoum state. Polygons less than 0.3km2 in area are removed. Holes smaller than 0.5km2 are filled.\n",
    "                                                                                                    #Subdivided using level 2 OCHA admin subdivions (with Karrari and Um Bada merged into one)\n",
    "    geziraCropland = ee.FeatureCollection(\"projects/seamproj01/assets/gezira_cropmask_v4\")  #Also based on Copernicus MDLC, cliped first with GlobCover dataset (via FAO LCLU). Then\n",
    "                                                                                            #Seperated based on position relative to Blue Nile (east or west)\n",
    "\n",
    "    sudanCropland = ee.FeatureCollection(\"projects/seamproj01/assets/sudan_cropmask_v1_2\") #A polygon dataset that demarcates Sudan's crop lands, per type (irritaged, rainfed), per federal state.\n",
    "    #This dataset were generated from Copernicus Moderate Dynamic Land Cover rasters combined with the ESA GlobCover (via FAO) dataset for irrigated land. The OCHA HDX Adminstrative Boudary data\n",
    "    #was used for state (and substate localities) borders. The resulting data was furhter processed to eliminate small polygons (<1 km2), fill holes (<=0.5 km2) and mesh simplification (QGIS\n",
    "    #grid method, 15 arsec threshold)\n",
    "\n",
    "    testAreaSamples = ee.FeatureCollection(\"projects/seamproj01/assets/test_samples\")   #50 samples covering testArea, with a string attribute NAME matching testArea's NAME attrib.\n",
    "    \n",
    "    #NOTE: If you are playing around with these datasets, note that their attributes are not all formatted the same way, so you will need to adjust e.g. subdivisionPropertyName for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Input / Model Parameters\n",
    "\n",
    "#===========================================================================================================================================\n",
    "#Model Parameters\n",
    "#===========================================================================================================================================\n",
    "\n",
    "#roi is a feature collection containing target polygons for the region of analysis, each polygon marks a homogenious zone (\"climatic divisions\" in the original\n",
    "#reasearch)\n",
    "#projectName is a string with which the name of the output raster will be prefixed\n",
    "roi : ee.FeatureCollection = testArea\n",
    "projectName : str = \"TestProj\" \n",
    "\n",
    "#Trial projects\n",
    "# roi = sudanCropland.filter(ee.Filter.eq(\"STATE\", \"Aj Jazirah\"))\n",
    "# projectName = \"Gezira\" \n",
    "\n",
    "# roi = sudanCropland.filter(ee.Filter.eq(\"STATE\", \"Aj Jazirah\")).filter(ee.Filter.eq(\"TYPE\", \"Irrigated\"))\n",
    "# projectName = \"GeziraIrrOnly\" \n",
    "\n",
    "# roi = sudanCropland.filter(ee.Filter.eq(\"STATE\", \"Khartoum\"))\n",
    "# projectName = \"Khartoum\"\n",
    "\n",
    "# roi = sudanCropland.filter(ee.Filter.eq(\"STATE\", \"Sennar\"))\n",
    "# projectName = \"Sennar\"\n",
    "\n",
    "# roi = sudanCropland.filter(ee.Filter.eq(\"STATE\", \"Sennar\")).filter(ee.Filter.inList(leftField = \"COMPOUND_I\", rightValue = [\"Abu Hujar_Rainfed\", \"Ad Dali_Rainfed\", \"Sennar_Rainfed\", \"Sinja_Rainfed\", \"Sinja_Irrigated\"]))\n",
    "# projectName = \"Sennar-W\"\n",
    "\n",
    "# roi = sudanCropland.filter(ee.Filter.eq(\"STATE\", \"Sennar\")).filter(ee.Filter.inList(leftField = \"COMPOUND_I\", rightValue = [\"At Dinder_Rainfed\", \"As Suki_Rainfed\", \"Sennar_Irrigated\", \"Sharg Sennar_Irrigated\"]))\n",
    "# projectName = \"Sennar-E\"\n",
    "\n",
    "#This it the name of the property (or \"attribute\", in GIS-speak) that will be used to split the resulting fallow timeseries raster into \"climatic divisions\".\n",
    "#NOTE: this is a required value. Even if your entire ROI is a single zone, you need to define some attribute here.\n",
    "subdivisionPropertyName : str = \"NAME\"\n",
    "# subdivisionPropertyName = \"COMPOUND_I\" #Supposed to be \"COMPOUND_ID\", but I forgot that shapefiles doesn't allow for feature name longer than 10 chars. Shapefiles must indeed die...\n",
    "\n",
    "#exportSingleRaster controls whether the resulting fallow timeseries will be split to smaller rasters (one for each climatic zone, using subdivisionPropertyName) or \n",
    "#one, big raster for the entire ROI.\n",
    "#NOTE: This would technically mean multiple smaller exports vs one big export. I'm not sure how this would affect the computations server side.\n",
    "exportSingleRaster : bool = True\n",
    "\n",
    "#This is the value for the z-score bellow which a pixel is considered fallow. Made a vairable here for caliberation purposes.\n",
    "#temporalZScoreThresholdMax = -3 #Original value used by Wallace et al.\n",
    "#temporalZScoreThresholdRange = -3 #Original value used by Wallace et al.\n",
    "temporalZScoreThresholdMax : float = -1.055\n",
    "temporalZScoreThresholdRange : float = -1.055\n",
    "\n",
    "#For spatial anomaly analysis, the max of a climate division for a given season is multiplied by this value.\n",
    "#spatialMedianMultiplierMax = 0.8 #Original value used by Wallace et al.\n",
    "#spatialMedianMultiplierMax = 0.8 #Original value used by Wallace et al.\n",
    "spatialMedianMultiplierMax : float = 0.855\n",
    "spatialMedianMultiplierRange : float = 0.855\n",
    "\n",
    "#time-series limits. All inclusive. \n",
    "#Months 1-12. User must make sure this range is covered by the selected dataset (MODIS / Sentinel)\n",
    "yearStart : int = 2013\n",
    "monthStart  : int = 1\n",
    "yearEnd  : int = 2025\n",
    "monthEnd  : int = 2\n",
    "\n",
    "#targetMonths are the months comprising the season for analysis.\n",
    "#WARNING! MUST BE 4 Values! Otherwise, the Temporal Anomaly Analysis component must be adjusted\n",
    "#WARNING! ORDER OF MONTHS MUST BE CHRONOLOGICAL! If the season is inter-annual, start with the months in the first year, then the second year\n",
    "#e.g. if the season starts on November, the list would be [11, 12, 1, 2]\n",
    "targetMonths : list[int] = [7, 8, 9, 10] #\"Summer\" season in Sudan (technically Autumn. Don't ask...). This covers the growth periods of crops such as sorghum.\n",
    "#targetMonths = [11, 12, 1, 2] #\"Winter\" season in Sudan (This one is indeed Winter). This covers growth periods of crops such as wheat\n",
    "\n",
    "#The original implementation of this workbook allows for use of one of three datasets, \"modis\" for MODIS terra SR, \"sentinel\" for Sentinel-2 MSI SR, and \"landsat\" for\n",
    "#LANDSAT 8 OLI Collection 2 Tier 2 SR. Set the string bellow to the desired dataset (obviously, mind your spelling)\n",
    "dataset : str = \"modis\"\n",
    "#NOTE: the Sentinel-2 L2A data used here miss some dates prior to dec 2018 for Gezira region. This is specific to GEE's version of 2A. Actual 2A on Copernicus Dataspace do cover this period.\n",
    "\n",
    "\n",
    "#The spatial anomaly analysis has two a reduceRegion operations, which are very expensive, and can cause computation to fail (due to hitting memory/time limits)\n",
    "#if the number of pixels in the rasters is too large, which is often the case with Sentinel data and large rois. reduceRegion() can be instructed to use a \"tileScale\"\n",
    "#to workaround memory limitation (consult the docs for more info), this value is assigned using reduceRegionTileScaleOverride\n",
    "#Value of the scale must be from 0.1 to 16.0. Default is 1.0.\n",
    "#NOTE: tile scale is basically a compromise factor, it shifts the bottleneck between processing time and memory requirements. Lower scales are faster but require a\n",
    "#lot of memory (i.e. you are bound to hit memory caps for large pixel count), while higher scales use less memory buy take longer to compute (so you are bound to hit\n",
    "#processing timeout limits).\n",
    "reduceRegionTileScaleOverride : float = 1.0\n",
    "# reduceRegionTileScaleOverride = 6.0\n",
    "\n",
    "#Simialiry, reduceReionNominalScaleOverride overrides the \"scale\" it computes with. If set to None, this script will use targetOutpuScale (which\n",
    "#is 10m for Sentinel, 250m for MODIS). If set to an integer, it will use that instead. Mostly useless for MODIS (should be kept as None), but for\n",
    "#Sentinel, greater values may be needed, depending on the size of the ROI. NOTE: this will affect the quality of the analysis.\n",
    "reduceRegionNominalScaleOverride : int | None = None\n",
    "\n",
    "#===========================================================================================================================================\n",
    "#Auto Calibration settings\n",
    "#===========================================================================================================================================\n",
    "#if autoCalibrateParams is true, the model will be calibrated using calibrationDataset.\n",
    "#calibrationAttribName is the attribute column name that contains integer marking whether land is fallow or not (0 = not fallow, 1 = fallow)\n",
    "#calibrationYear is the year which the values in ISFALLOW represent\n",
    "#NOTE: currently, auto calibration requires manual cell execution (because it involves waiting for GEE to finish an export job and Setting this to true requires\n",
    "#manually executing this workbook to the cell with \"#Auto-calibration first cell\" heading), waiting for export to finish, then executing the remaining cells.\n",
    "#NOTE: when auto calibrating, the final fallow timeseries will not be exported. Results for calibration (accuracy + parameters) will be output in the last\n",
    "#relevant cell bellow.\n",
    "autoCalibrateParams : bool = False\n",
    "calibrationDataset : ee.FeatureCollection = testAreaSamples #Note: ensure that ALL the points ins this dataset fall within the your roi polygons (and subsequent rasters), else you'll get exceptions\n",
    "calibrationAttribName : str = \"ISFALLOW\"\n",
    "calibrationYear : int = 2019\n",
    "\n",
    "#The calibration needs to export two images and store them. Exported to pathPrefix + \"tempAn\", and pathPrefix + \"spStat\"\n",
    "#Adjust pathPrefix to point to your GEE project's assets (i.e. replace \"seamproj01\" with the name of your project)\n",
    "pathPrefix = \"projects/seamproj01/assets/\"\n",
    "\n",
    "#Differential Evolution parameters:\n",
    "popSize: int = 30 #Population size. Minimum = 4 Recommended estimate is 10 * parameters. e.g. for four params: 10 * 4 = 40\n",
    "crossOverProb: float = 0.9 #Cross Over Probability. Should be between 0.0 and 1.0. Recommended default = 0.9\n",
    "diffWeight: float = 0.8 #Differential Weight. Between 0.0 and 2.0. Recommended default = 0.8\n",
    "coupleParams: bool = True  #if True, zScore params shall be coupled, as well as spatialMedianMultipliers (i.e. 2 params instead of 4). Make sure to adjust \n",
    "                            #paramRanges bellow accordingely. Also note: coupling params means you probably could get away with a smaller popSize\n",
    "paramRanges: list[list[float]] = [[-1.0, -4.0], [-1.0, -4.0], [0.5, 1.5], [0.5, 1.5]] #Allowble range for each parameter.\n",
    "                                                                    #order = temporalZScoreThresholdMax, temporalZScoreThresholdRange, spatialMedianMultiplierMax,\n",
    "                                                                    #and spatialMedianMultiplierRange\n",
    "                                                                    #if coupling params, order = [zScoreThresholds, spatialMedianMultiliers]\n",
    "diffEvSeed: int = 485138463513684685 #seed for the randomizer. Ensures reproducability. Setting to None makes each run different than preceding one\n",
    "minIterations: int = 15 #Optimisation shall proceed for iteratios no less than this number\n",
    "maxIterations: int = 30 #Optimisation shall break after this number of iterations, regardless of results\n",
    "minChangeThreshold: float = 0.01 #minimum (percentage) change in objective function between iterations before stopping optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input ImageCollections and Dates Preprocessing\n",
    "\n",
    "#===========================================================================================================================================\n",
    "#Helper methods\n",
    "#===========================================================================================================================================\n",
    "\n",
    "#This function is especially usefull for sentinel data. To try and improve performance a bit, we'll limit the collection to only target months, using ee.Filter.calendarRange(start, end, field)\n",
    "#problem is, targetMonths may no be in order (consider inter-annual years), so we can't just take first and last entry. can't take max or min either else it would defeat purpose (min/max\n",
    "#of [11, 12, 1, 2] would result in twelve months).\n",
    "def FilterCollectionForPeriod(  col : ee.ImageCollection,\n",
    "                                startDate : str,\n",
    "                                endDate : str,\n",
    "                                targetMonths : list) -> ee.ImageCollection:\n",
    "    periodStart = 0\n",
    "    periodEnd = 0\n",
    "    periods = []\n",
    "    for i in range (1, len(targetMonths)):\n",
    "        if (targetMonths[i] > targetMonths[i - 1]):\n",
    "            periodEnd = i\n",
    "        else:\n",
    "            periods.append([targetMonths[periodStart], targetMonths[periodEnd]])\n",
    "            periodStart = periodEnd = i\n",
    "\n",
    "    periods.append([targetMonths[periodStart], targetMonths[periodEnd]])\n",
    "\n",
    "    monthsFilter = ee.Filter.calendarRange(periods[0][0], periods[0][1], \"month\")\n",
    "    for i in range (1, len(periods)):\n",
    "        monthsFilter = ee.Filter.Or(monthsFilter, ee.Filter.calendarRange(periods[i][0], periods[i][1], \"month\"))\n",
    "\n",
    "    return col.filterDate(startDate, endDate).filter(monthsFilter)\n",
    "\n",
    "#Clipping image with high vertex-count polygons can take a LOT of time. So, we clip our rasters to the bounding box of these polygons, and suffer processing the extra\n",
    "#pixels. From quick testing, this is still much faster than clipping to exact geometry. Computing NDVI TS for khartoum (v3) dataset (2019-2023 winter seasons) took only\n",
    "#2 hours with the new clipping method. With the original, exact clipping, it wasn't finished even after 17hrs (canceled it at that point)\n",
    "#Note that this doesn't affect the clipping optimisation in the spatial anomaly analysis component, neither the reduceRegion(s) used in it\n",
    "def ClipCollectionToCollection(imageCol : ee.ImageCollection, featureCol : ee.FeatureCollection):\n",
    "    clipGeometry = featureCol.map(lambda feature : ee.Feature(feature.geometry().convexHull())).geometry() \n",
    "    return imageCol.map(lambda image : image.clip(clipGeometry).copyProperties(image, image.propertyNames()))\n",
    "\n",
    "def ProcessMODISCollection( col : ee.ImageCollection,\n",
    "                            roi : ee.FeatureCollection,\n",
    "                            startDate : str,\n",
    "                            endDate : str,\n",
    "                            targetMonths : list) -> ee.ImageCollection:\n",
    "    \n",
    "    #mappable functions (leaving them scoped inside the MODIS function because similarily named ones with different implementation exist for Sentinel as well)\n",
    "    def MaskPoortQualityPixels(img : ee.Image) -> ee.Image:\n",
    "        qaBand = img.select(\"State\")\n",
    "        #TODO add snow/ice masking\n",
    "        mask = qaBand.bitwiseAnd(3).eq(0) #pixel is clear from cloud (bits 0 and 1)\n",
    "        mask = mask.And(qaBand.bitwiseAnd(4).eq(0)) #not cloud shadow (bit 2)\n",
    "        mask = mask.And(qaBand.bitwiseAnd(768).eq(0)) #no cirrus (bits 8 and 9)\n",
    "        return img.updateMask(mask)\n",
    "    \n",
    "    output = FilterCollectionForPeriod(col, startDate, endDate, targetMonths)\n",
    "    output = output.filterBounds(roi.geometry())\n",
    "    #output = output.map(lambda image : image.clip(roi).copyProperties(image, image.propertyNames()))\n",
    "    output = ClipCollectionToCollection(output, roi)\n",
    "    output = output.map(MaskPoortQualityPixels)\n",
    "    output = output.map(lambda image : image.normalizedDifference([\"sur_refl_b02\", \"sur_refl_b01\"]).rename(\"NDVI\").copyProperties(image, image.propertyNames()))\n",
    "    return output\n",
    "\n",
    "def ProcessSentinelCollection(  col : ee.ImageCollection,\n",
    "                                roi : ee.FeatureCollection,\n",
    "                                startDate : str,\n",
    "                                endDate : str,\n",
    "                                targetMonths : list) -> ee.ImageCollection:\n",
    "    \n",
    "    def MaskPoortQualityPixels(img : ee.Image) -> ee.Image:\n",
    "        qaBand = img.select(\"SCL\")\n",
    "        #Unlike MODIS, the quality band used here, \"SCL,\" contains a single int meant to be interpreted as a single int.\n",
    "        #https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n",
    "        \n",
    "        mask = qaBand.eq(3) #cloud shadows\n",
    "        mask = mask.And(qaBand.eq(6)) #water\n",
    "        mask = mask.And(qaBand.eq(8)) #medium probability clouds\n",
    "        mask = mask.And(qaBand.eq(9)) #high probability clouds\n",
    "        mask = mask.And(qaBand.eq(10)) #thin cirrus\n",
    "        mask = mask.And(qaBand.eq(11)) #snow/ice\n",
    "\n",
    "        mask = mask.neq(1) #flip so it masks out the above\n",
    "        \n",
    "        return img.updateMask(mask)\n",
    "    \n",
    "    #filter then process the output collection\n",
    "    output = FilterCollectionForPeriod(col, startDate, endDate, targetMonths)\n",
    "    output = output.filterBounds(roi.geometry())\n",
    "    output = output.select(\"B4\", \"B8\", \"SCL\") #grasping at straws trying to make this thing run faster... TODO experiment to see if this actually has an effect\n",
    "    #output = output.map(lambda image : image.clip(roi).copyProperties(image, image.propertyNames()))\n",
    "    output = ClipCollectionToCollection(output, roi)\n",
    "    output = output.map(MaskPoortQualityPixels)\n",
    "    output = output.map(lambda image : image.normalizedDifference([\"B8\", \"B4\"]).rename(\"NDVI\").copyProperties(image, image.propertyNames()))\n",
    "    return output\n",
    "\n",
    "#TODO The MaskPoorQualityPixels() method for landsat is very similar to modis. Could use some DRYing.\n",
    "#TODO the bodies of these three functions is highly similar. DRY it.\n",
    "def ProcessLandsatCollection    (col : ee.ImageCollection,\n",
    "                                roi : ee.FeatureCollection,\n",
    "                                startDate : str,\n",
    "                                endDate : str,\n",
    "                                targetMonths : list) -> ee.ImageCollection:\n",
    "    \n",
    "    def MaskPoortQualityPixels(img : ee.Image) -> ee.Image:\n",
    "        qaBand = img.select(\"QA_PIXEL\")\n",
    "        \n",
    "        mask = qaBand.bitwiseAnd(4).eq(0) #no cirrus detected (bit 2)\n",
    "        # mask = mask.And(qaBand.bitwiseAnd(64).eq(0)) #pixel set to clear (no clouds) (bit 6) #is this for the entire image, not the pixel?\n",
    "        mask = mask.And(qaBand.bitwiseAnd(8).eq(0)) #no cloud (bit 3)\n",
    "        mask = mask.And(qaBand.bitwiseAnd(16).eq(0)) #no cloud shadow (bit 4)\n",
    "        mask = mask.And(qaBand.bitwiseAnd(128).eq(0)) #no water (bit 7)\n",
    "        return img.updateMask(mask)\n",
    "    \n",
    "    #filter then process the output collection\n",
    "    output = FilterCollectionForPeriod(col, startDate, endDate, targetMonths)\n",
    "    output = output.filterBounds(roi.geometry())\n",
    "    output = output.select(\"SR_B4\", \"SR_B5\", \"QA_PIXEL\")\n",
    "    output = ClipCollectionToCollection(output, roi)\n",
    "    output = output.map(MaskPoortQualityPixels)\n",
    "    output = output.map(lambda image : image.multiply(0.0000275).subtract(0.2).normalizedDifference([\"SR_B5\", \"SR_B4\"]).rename(\"NDVI\").copyProperties(image, image.propertyNames()))#.set({\"_year\" : ee.Date.format(image.get(\"system:time_start\"), \"YYYY\")}))\n",
    "    return output\n",
    "    \n",
    "\n",
    "#===========================================================================================================================================\n",
    "#Preprocessing\n",
    "#===========================================================================================================================================\n",
    "\n",
    "dateStart = f\"{yearStart}-{monthStart}-1\"\n",
    "dateEnd = f\"{yearEnd}-{monthEnd + 1}-1\" if monthEnd < 12 else  f\"{yearEnd + 1}-1-1\"\n",
    "\n",
    "processedCol = None\n",
    "\n",
    "if dataset == \"sentinel\":\n",
    "    processedCol = ProcessSentinelCollection( ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\"), roi, dateStart, dateEnd, targetMonths)\n",
    "    targetOutputScale = 10\n",
    "elif dataset == \"modis\":\n",
    "    processedCol = ProcessMODISCollection(ee.ImageCollection(\"MODIS/061/MOD09Q1\"), roi, dateStart, dateEnd, targetMonths)\n",
    "    targetOutputScale = 250\n",
    "elif dataset == \"landsat\":\n",
    "    processedCol = ProcessLandsatCollection(ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\"), roi, dateStart, dateEnd, targetMonths)\n",
    "    targetOutputScale = 30\n",
    "\n",
    "ndviTS = processedCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this block, we generte the timeseries (monthly max NDVI and NDVI range) and the statistics for the \"pure crop\" signal.\n",
    "#Outputs of this block are two ImageCollections: \"timeseries,\" and \"pureCropNDVI\"\n",
    "#Note: \"year\" here is used to refer to the year of the start of the season, no the calendar year of the month. This is important for inter-annual seasons (e.g. Nov through\n",
    "#Feb). \n",
    "    #timseries contains images for each season.\n",
    "        #Each Image has number of bands equal to twice the number of targetMonths (2x4 = 8).\n",
    "        #Each Image has a property \"year\" for the season's year.\n",
    "        #Each band is named \"month_x_max\" or \"month_x_range\", where x is the month's number; \"max\" and \"range\" denote whether it encodes the maximum monthly ndvi or ndvi monthly range.\n",
    "    #pureCropNDVI contains images for each month in the targetMonths (total = 4)\n",
    "        #each image has 4 bands: \"month_x_max_mean\" or \"month_x_max_stdDev\" (and similarily for range).\n",
    "        #each image has a property \"month\" with month's number\n",
    "\n",
    "timeSeries = ee.List([])\n",
    "\n",
    "lastCompleteSeason = yearStart\n",
    "\n",
    "for year in range(yearStart, yearEnd + 1):\n",
    "    \n",
    "    yearTS = ee.List([])\n",
    "    isCompleteSeason = True\n",
    "    \n",
    "    lastMonth = 0\n",
    "    calendarYear = year\n",
    "    lastCompleteSeason = year\n",
    "\n",
    "    for month in targetMonths:\n",
    "        if (month < lastMonth):\n",
    "            calendarYear += 1\n",
    "        lastMonth = month\n",
    "        \n",
    "        if ((calendarYear * 100) + month > (yearEnd * 100) + monthEnd):\n",
    "            isCompleteSeason = False\n",
    "            break\n",
    "        \n",
    "        \n",
    "        subPeriodStart = f\"{calendarYear}-{month}-1\"\n",
    "        subPeriodEnd = f\"{calendarYear}-{month + 1}-1\" if month < 12 else  f\"{calendarYear + 1}-1-1\"\n",
    "        subCol = ndviTS.filterDate(subPeriodStart, subPeriodEnd)\n",
    "        \n",
    "        minMaxNDVI = subCol.reduce(ee.Reducer.minMax())\n",
    "\n",
    "        monthMax = minMaxNDVI.select(\"NDVI_max\").rename(\"max\")\n",
    "        monthRange = minMaxNDVI.select(\"NDVI_max\").subtract(minMaxNDVI.select(\"NDVI_min\")).rename(\"range\")\n",
    "        yearTS = yearTS.add(ee.Image([monthMax, monthRange]).set({\"system:index\" : f\"month_{month}\"}))\n",
    "    \n",
    "    if(not isCompleteSeason):\n",
    "        lastCompleteSeason += -1\n",
    "        print (f\"WARNING! Reached an incomplete season at year {lastCompleteSeason + 1}, timeseries will end at {lastCompleteSeason}\")\n",
    "        break\n",
    "    \n",
    "    yearTS = ee.ImageCollection(yearTS).toBands().set({\"year\" : year, \"system:index\" : ee.String(\"year_\").cat(str(year))})\n",
    "    timeSeries = timeSeries.add(yearTS)\n",
    "\n",
    "timeSeries = ee.ImageCollection(timeSeries)\n",
    "\n",
    "##uncomment this part to export the ndviTS. Not very recommended for Sentinel 2 data for large areas (produces rasters between 500~900MB for Gezira dataset)...\n",
    "##to reduce output size, the data is scaled by 100 then rounded off, and stored as an unsigned, 8bit int which range 0 to 255, but because ndvi ts ranges from 0 \n",
    "##to 1 (zero because we masked out water), the resulting raster would range from 0 to 100. Also note the reduced precision this introduces (shouldn't matter\n",
    "##much anyway)\n",
    "##TODO Extra precision could be obtained by using the full 0~255 range by multiplying Ndvi with 255 then truncating decimals. This would allow for the storage\n",
    "##of down to 0.004 difference (more precise than 0.01 of range 0~100) Retrival is carried out by subdividing by 255.\n",
    "##TODO adjust code so bands would have name of year.\n",
    "##TODO seperate output images for this the same as you do fallowTS bellow.\n",
    "##TODO export max and range timeseries seperately\n",
    "# task = ee.batch.Export.image.toDrive(\n",
    "#     image = timeSeries.select([f\"month_{targetMonths[0]}_max\", f\"month_{targetMonths[1]}_max\", f\"month_{targetMonths[2]}_max\", f\"month_{targetMonths[3]}_max\"]).toBands().clip(roi.geometry()).multiply(ee.Image(ee.Number(100))).toUint8(),\n",
    "#     description = f\"{projectName}_maxNDVI_TS_{yearStart}-{yearEnd}_season_{targetMonths[0]}-{targetMonths[1]}-{targetMonths[2]}-{targetMonths[3]}\",\n",
    "#     # image = timeSeries.select([f\"month_{targetMonths[0]}_range\", f\"month_{targetMonths[1]}_range\", f\"month_{targetMonths[2]}_range\", f\"month_{targetMonths[3]}_range\"]).toBands().clip(roi.geometry()).multiply(ee.Image(ee.Number(100))).toUint8(),\n",
    "#     # description = f\"{projectName}_rangeNDVI_TS_{yearStart}-{yearEnd}_season_{targetMonths[0]}-{targetMonths[1]}-{targetMonths[2]}-{targetMonths[3]}\",\n",
    "#     #folder='ee_export',\n",
    "#     region = roi.geometry(),\n",
    "#     scale = targetOutputScale,\n",
    "#     crs = 'EPSG:4326',\n",
    "#     maxPixels = 500000000,\n",
    "#     fileFormat = 'GeoTIFF',\n",
    "#     formatOptions = {\n",
    "#         'noData': -9999\n",
    "# })\n",
    "# task.start()\n",
    "\n",
    "\n",
    "pureCropNDVI = ee.List([])\n",
    "\n",
    "for month in targetMonths:\n",
    "    prefix = f\"month_{month}_\"\n",
    "    thisMonthTS = timeSeries.select([prefix + \"max\", prefix + \"range\"])\n",
    "\n",
    "    monthMedian = thisMonthTS.reduce(ee.Reducer.median()).rename([prefix + \"max\", prefix + \"range\"]).set({\"month\" : month})\n",
    "    \n",
    "    meanStdReducer = ee.Reducer.mean().combine(ee.Reducer.stdDev(), sharedInputs = True)\n",
    "\n",
    "    monthPureCropNDVI = thisMonthTS.map(lambda img : img.updateMask(img.gte(monthMedian)))\n",
    "    monthPureCropNDVI = monthPureCropNDVI.reduce(meanStdReducer).set(\"month\", month)\n",
    "\n",
    "    pureCropNDVI = pureCropNDVI.add(monthPureCropNDVI)\n",
    "\n",
    "pureCropNDVI = ee.ImageCollection(pureCropNDVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block computes the temporal anomalies (z-scores) for each season. The output is an ImageCollection called temporalAnomalies, containing Images representing each season.\n",
    "    #Each Image has number of bands equal to twice the number of targetMonths (2x4 = 8).\n",
    "    #Each Image has a property \"year\" for the season's year.\n",
    "    #Each band is named \"month_x_max\" or \"month_x_range\", respectively for the z-scores for max NDVI and NDVI range, for each target month.\n",
    "\n",
    "def ComputeTemporalAnomalies(image : ee.Image) -> ee.Image:\n",
    "    seasonAnomalies = ee.List([])\n",
    "\n",
    "    for month in targetMonths:\n",
    "        prefix = f\"month_{month}_\"\n",
    "        pureCropSignal = pureCropNDVI.filter(ee.Filter.eq(\"month\", month)).first()\n",
    "        \n",
    "        taMax = image.select(prefix + \"max\").subtract(pureCropSignal.select(prefix + \"max_mean\")).divide(pureCropSignal.select(prefix + \"max_stdDev\")).rename(\"max\")\n",
    "        taRange = image.select(prefix + \"range\").subtract(pureCropSignal.select(prefix + \"range_mean\")).divide(pureCropSignal.select(prefix + \"range_stdDev\")).rename(\"range\")\n",
    "\n",
    "        ta = ee.Image([taMax, taRange]).set({\"system:index\" : f\"month_{month}\"})\n",
    "        seasonAnomalies = seasonAnomalies.add(ta)\n",
    "    \n",
    "    return ee.ImageCollection(seasonAnomalies).toBands().set({\"year\" : image.get(\"year\")})\n",
    "    \n",
    "temporalAnomalies = timeSeries.map(ComputeTemporalAnomalies)\n",
    "\n",
    "##uncomment this part to export the anomaly series.\n",
    "##similarily to ndvi timeseries export, the data is scaled by 100 the rounded off, and stored as a 16bit int with range -32,768 to 32,767\n",
    "##since z score shouldn't go far from zero (negative or positive single digits, typically), this range should be more than adequte.\n",
    "##Note: gee refuses to set image index (its name) inside map(). So the output image will be prefixed with integers starting from 0, instead of year number.\n",
    "##TODO export max and range series seperately\n",
    "# task = ee.batch.Export.image.toDrive(\n",
    "#     image = temporalAnomalies.toBands().clip(roi.geometry()).multiply(ee.Image(ee.Number(100))).toInt16(),\n",
    "#     description = f\"{projectName}_temporalAnomalies_{yearStart}-{yearEnd}_season_{targetMonths[0]}-{targetMonths[1]}-{targetMonths[2]}-{targetMonths[3]}\",\n",
    "#     #folder='ee_export',\n",
    "#     region = roi.geometry(),\n",
    "#     scale = targetOutputScale, #WARNING! The zscore data isn't very compressible, even when reduced to int16. Using large scales (less than 30m) with large areas (gezira example) results in rasters 11GB in size\n",
    "#     crs = 'EPSG:4326',\n",
    "#     maxPixels = 500000000,\n",
    "#     fileFormat = 'GeoTIFF',\n",
    "#     formatOptions = {\n",
    "#         'noData': -9999\n",
    "# })\n",
    "# task.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal analysis component.\n",
    "\n",
    "#mapable function, mapped over temporalAnomalies collection\n",
    "def TemporalAnomalyAnalysis(image : ee.Image) -> ee.Image: #to be mapped over temporalAnomalies collection\n",
    "    isFallow_1 = image.select(f\"month_{targetMonths[0]}_max\").lt(temporalZScoreThresholdMax).And(image.select(f\"month_{targetMonths[1]}_max\").lt(temporalZScoreThresholdMax)).And(image.select(f\"month_{targetMonths[2]}_max\").lt(temporalZScoreThresholdMax))\n",
    "    isFallow_1 = isFallow_1.Or(image.select(f\"month_{targetMonths[1]}_max\").lt(temporalZScoreThresholdMax).And(image.select(f\"month_{targetMonths[2]}_max\").lt(temporalZScoreThresholdMax)).And(image.select(f\"month_{targetMonths[3]}_max\").lt(temporalZScoreThresholdMax)))\n",
    "    isFallow_1 = isFallow_1.rename(\"max\")\n",
    "\n",
    "    isFallow_2 = image.select(f\"month_{targetMonths[0]}_range\").lt(temporalZScoreThresholdRange).And(image.select(f\"month_{targetMonths[1]}_range\").lt(temporalZScoreThresholdRange)).And(image.select(f\"month_{targetMonths[2]}_range\").lt(temporalZScoreThresholdRange))\n",
    "    isFallow_2 = isFallow_2.Or(image.select(f\"month_{targetMonths[1]}_range\").lt(temporalZScoreThresholdRange).And(image.select(f\"month_{targetMonths[2]}_range\").lt(temporalZScoreThresholdRange)).And(image.select(f\"month_{targetMonths[3]}_range\").lt(temporalZScoreThresholdRange)))\n",
    "    isFallow_2 = isFallow_2.rename(\"range\")\n",
    "\n",
    "    return ee.Image([isFallow_1, isFallow_2]).set({\"year\" : image.get(\"year\")})\n",
    "\n",
    "#isFallow_TA the two questions for the temporal anomalies\n",
    "isFallow_TA = temporalAnomalies.map(TemporalAnomalyAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial analysis component.\n",
    "\n",
    "#mapable function, mapped over timeseries collection to generate input for spatial anomally analysis\n",
    "def SpatialStatistics(image : ee.Image) -> ee.Image:\n",
    "    maxOfMax = ee.List([])\n",
    "    maxOfRange = ee.List([])\n",
    "    \n",
    "    spatialMedianMax = ee.FeatureCollection([])\n",
    "    spatialMedianRange = ee.FeatureCollection([])\n",
    "\n",
    "    for month in targetMonths:\n",
    "        name = f\"month_{month}\"\n",
    "        _maxOfMax : ee.Image = image.select(name + \"_max\").rename(\"max\")\n",
    "        _maxOfRange : ee.Image = image.select(name + \"_range\").rename(\"range\")\n",
    "\n",
    "        maxOfMax = maxOfMax.add(_maxOfMax)\n",
    "        maxOfRange = maxOfRange.add(_maxOfRange)\n",
    "        \n",
    "        zoneMaxMedian = _maxOfMax.reduceRegions(collection = roi,\n",
    "                                                reducer = ee.Reducer.median(),\n",
    "                                                scale = targetOutputScale if reduceRegionNominalScaleOverride is None else reduceRegionNominalScaleOverride,\n",
    "                                                crs = \"EPSG:4326\",\n",
    "                                                tileScale = reduceRegionTileScaleOverride)\n",
    "\n",
    "        zoneRangeMedian =  _maxOfRange.reduceRegions(   collection = roi,\n",
    "                                                        reducer = ee.Reducer.median(),\n",
    "                                                        scale = targetOutputScale if reduceRegionNominalScaleOverride is None else reduceRegionNominalScaleOverride,\n",
    "                                                        crs = \"EPSG:4326\",\n",
    "                                                        tileScale = reduceRegionTileScaleOverride)\n",
    "        \n",
    "        spatialMedianMax = spatialMedianMax.merge(zoneMaxMedian)\n",
    "        spatialMedianRange = spatialMedianRange.merge(zoneRangeMedian)\n",
    "        \n",
    "    maxOfMax = ee.ImageCollection(maxOfMax).reduce(ee.Reducer.max(), parallelScale = 4).rename(\"maxOfMax\")\n",
    "    maxOfRange = ee.ImageCollection(maxOfRange).reduce(ee.Reducer.max(), parallelScale = 4).rename(\"maxOfRange\")\n",
    "\n",
    "    zonesMedians = roi.map(lambda feature : feature.set({\"medianOfMax\" : ee.Number(spatialMedianMax.filter(ee.Filter.eq(subdivisionPropertyName, feature.get(subdivisionPropertyName))).aggregate_array(\"median\").reduce(ee.Reducer.max()))}))\n",
    "    zonesMedians = zonesMedians.map(lambda feature : feature.set({\"medianOfRange\" : ee.Number(spatialMedianRange.filter(ee.Filter.eq(subdivisionPropertyName, feature.get(subdivisionPropertyName))).aggregate_array(\"median\").reduce(ee.Reducer.max()))}))\n",
    "    \n",
    "    #mappable function, mapped over a list of features to generate a list of fixed value rasters (with two bands) clipped to the features.\n",
    "    def RasterFromVectorStats(feature : ee.Feature) -> ee.Image:\n",
    "        feature = ee.Feature(feature)\n",
    "        medianOfMax = ee.Image(ee.Number(feature.get(\"medianOfMax\"))).clip(feature.geometry()).toFloat()\n",
    "        medianOfRange = ee.Image(ee.Number(feature.get(\"medianOfRange\"))).clip(feature.geometry()).toFloat()\n",
    "        return ee.Image([medianOfMax, medianOfRange]).rename([\"medianOfMax\", \"medianOfRange\"])\n",
    "    \n",
    "    #Generate a single image (with two bands) from the zonesMedians\n",
    "    spatialMedians = ee.ImageCollection(zonesMedians.toList(zonesMedians.size()).map(RasterFromVectorStats)).mosaic()\n",
    "\n",
    "    return ee.Image([maxOfMax, maxOfRange,\n",
    "                     ee.Image(spatialMedians.select(\"medianOfMax\")), \n",
    "                     ee.Image(spatialMedians.select(\"medianOfRange\"))]).set({\"year\" : image.get(\"year\")})\n",
    "\n",
    "#mapable function, mapped over spatial statistics to generate the isFallow rasters based on spatial anomalies\n",
    "def SpatialAnomalyAnalysis(image : ee.Image) -> ee.Image:\n",
    "    maxOfMax = image.select(\"maxOfMax\")\n",
    "    maxOfRange = image.select(\"maxOfRange\")\n",
    "    spatialMedianMax = image.select(\"medianOfMax\")\n",
    "    spatialMedianRange = image.select(\"medianOfRange\")\n",
    "\n",
    "    isFallow_3 = maxOfMax.lt(spatialMedianMax.multiply(ee.Number(spatialMedianMultiplierMax))).rename(\"max\")\n",
    "    isFallow_4 = maxOfRange.lt(spatialMedianRange.multiply(ee.Number(spatialMedianMultiplierRange))).rename(\"range\")\n",
    "\n",
    "    return ee.Image([isFallow_3, isFallow_4]).set({\"year\" : image.get(\"year\")})\n",
    "\n",
    "\n",
    "#isFallow_SA the two questions for the spatial anomalies\n",
    "spatialStats = timeSeries.map(SpatialStatistics)\n",
    "isFallow_SA = spatialStats.map(SpatialAnomalyAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final analysis.\n",
    "#For a pixel to qualify as fallow, it has to be classified as such in at least two of either the temporal or spatial checks.\n",
    "\n",
    "def ComputeFallowTS(isFallow_TA, isFallow_SA):\n",
    "    isFallowComponents = ee.List([])\n",
    "    for year in range (yearStart, yearEnd + 1):\n",
    "        if ((year * 100) + targetMonths[0] > (yearEnd * 100) + monthEnd):\n",
    "                break\n",
    "                \n",
    "        yearComponents = ee.List([]).add(isFallow_TA.filter(ee.Filter.eq(\"year\", year)).first().set({\"system:index\" : f\"{year}_TA\"}))\n",
    "        yearComponents = yearComponents.add(isFallow_SA.filter(ee.Filter.eq(\"year\", year)).first().set({\"system:index\" : f\"{year}_SA\"}))\n",
    "\n",
    "        yearComponents = ee.ImageCollection(yearComponents).toBands().set({\"year\" : str(year)})\n",
    "        isFallowComponents = isFallowComponents.add(yearComponents)\n",
    "\n",
    "    isFallowComponents = ee.ImageCollection(isFallowComponents)\n",
    "    \n",
    "    return isFallowComponents.map(lambda img : img.reduce(ee.Reducer.sum()).gte(ee.Number(2)).rename(ee.String(img.get(\"year\")))).toBands()\n",
    "\n",
    "\n",
    "fallowTS = ComputeFallowTS(isFallow_TA, isFallow_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-calibration first cell\n",
    "#Cache temporatlAnomalies and spatialStats (which are expensive to compute) as an asset to be used for optimisation\n",
    "\n",
    "#NOTE: the \"task.start()\" calls are commented out for testing, since this entire workbook may be recomputed multiple times during optimisation (either due to\n",
    "#optimisation param changes, debugging, etc). Uncomment those callse and run this cell ONCE. Wait for the export to finish (check your GEE or Google Cloud account's\n",
    "#task manager), then comment those lines out.\n",
    "#You will need to rerun this for every roi change.\n",
    "#You will need to rerun this if you changed the target months, or if you expanded your year ranges (if new range is within than old one, you don't need to recompute)\n",
    "\n",
    "#TODO gee complains about overwriting assets. Is there a flag in the export to force it? If not, think of an elegant way to handle this.\n",
    "\n",
    "#To make life a little bit easier, clip the rasters above to a small area around the sampling points, since we don't need the whole region for calibration,\n",
    "#which is what we are doing if we are in this block\n",
    "\n",
    "clipper = testAreaSamples.map(lambda point : point.buffer(targetOutputScale)) #ideally, we would clip only to the sampled pixels, but getting their exact positions may be time consuming\n",
    "#buffer on a point returns a circle, let's convert it to a rectangle. Not necessary, but I like rects better for these kinda things :) Using the circles bounds simplifis this.\n",
    "clipper = clipper.map(lambda circle : ee.Geometry.Rectangle(ee.Array.cat(circle.geometry().bounds().coordinates(),1).slice(0, 0, 3, 2).reshape([-1]).toList()))\n",
    "\n",
    "if autoCalibrateParams:\n",
    "    task = ee.batch.Export.image.toAsset(temporalAnomalies.toBands().clip(clipper.geometry()), \n",
    "                                        \"temporalAnomalies\",  \n",
    "                                        pathPrefix + \"tempAn\", \n",
    "                                        scale = targetOutputScale, \n",
    "                                        maxPixels= 3e8)\n",
    "    # task.start() #NOTE uncomment this to generate the temporal anomaly raster for calibration, then comment out.\n",
    "\n",
    "    task = ee.batch.Export.image.toAsset(spatialStats.toBands().clip(clipper.geometry()), \n",
    "                                        \"spatialStats\",  \n",
    "                                        pathPrefix + \"spStat\", \n",
    "                                        scale = targetOutputScale, \n",
    "                                        maxPixels= 3e8)\n",
    "\n",
    "    # task.start() #NOTE uncomment this to generate the spatial statistics raster for calibration, then comment out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate temporaAnomalies and spatialStats from cached rasters\n",
    "\n",
    "if autoCalibrateParams:\n",
    "    tempAnImg = ee.Image(pathPrefix + \"tempAn\")\n",
    "    spStatImg = ee.Image(pathPrefix + \"spStat\")\n",
    "\n",
    "    temporalAnomalies = ee.List([])\n",
    "    spatialStats = ee.List([])\n",
    "\n",
    "    spStatsNames = [\"maxOfMax\", \"maxOfRange\", \"medianOfMax\", \"medianOfRange\"]\n",
    "    taBandNames = [] #the process bellow breaks the band naming (prefexes an integer), generate this list for easier renaming bellow.\n",
    "    for m in targetMonths:\n",
    "        taBandNames.append(f\"month_{m}_max\")\n",
    "        taBandNames.append(f\"month_{m}_range\")\n",
    "\n",
    "    yearsList = [int(i[5:9]) for i in tempAnImg.bandNames().getInfo()]  #workaround for an issue introduced by inter-annual seasons.\n",
    "\n",
    "    for year in range(yearStart, yearEnd + 1):\n",
    "        if year not in yearsList:  #workaround for an issue introduced by inter-annual seasons.\n",
    "            continue\n",
    "\n",
    "        yearImgTA = ee.List([])\n",
    "        #temporal anomalies are by month, so we loop over targetMonths\n",
    "        for month in targetMonths:\n",
    "            bandNameMax = f\"year_{year}_month_{month}_max\"\n",
    "            bandNameRange = f\"year_{year}_month_{month}_range\"\n",
    "            \n",
    "            yearImgTA = yearImgTA.add(tempAnImg.select(bandNameMax).rename([f\"month_{month}_max\"]))\n",
    "            yearImgTA = yearImgTA.add(tempAnImg.select(bandNameRange).rename([f\"month_{month}_range\"]))\n",
    "\n",
    "        #spatialStats are for an entire year, not by month.\n",
    "        yearImgSS = ee.ImageCollection([spStatImg.select(f\"year_{year}_{statName}\") for statName in spStatsNames]).toBands().rename(spStatsNames).set({\"year\" : year})\n",
    "\n",
    "        temporalAnomalies = temporalAnomalies.add(ee.ImageCollection(yearImgTA).toBands().set({\"year\" : year}).rename(taBandNames))\n",
    "        spatialStats = spatialStats.add(yearImgSS)\n",
    "\n",
    "    temporalAnomalies = ee.ImageCollection(temporalAnomalies)\n",
    "    spatialStats = ee.ImageCollection(spatialStats)\n",
    "\n",
    "    # #test\n",
    "    # print (\"for tempAn\")\n",
    "    # print (temporalAnomalies.first().bandNames().getInfo())\n",
    "    # print (temporalAnomalies.aggregate_array(\"year\").getInfo())\n",
    "    # print (\"for spStat\")\n",
    "    # print (spatialStats.first().bandNames().getInfo())\n",
    "    # print (spatialStats.aggregate_array(\"year\").getInfo())\n",
    "\n",
    "    # map.addLayer(clipper, name=\"clipper\")\n",
    "    # map.addLayer(tempAnImg, name=\"tempAn\")\n",
    "    # map.addLayer(spStatImg, name=\"spStat\")\n",
    "    # map.addLayer(testAreaSamples, name=\"samples\")\n",
    "\n",
    "    # map.centerObject(testAreaSamples.geometry())\n",
    "    # map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autocaliberation using Differential Evolution\n",
    "#https://en.wikipedia.org/wiki/Differential_evolution\n",
    "\n",
    "if autoCalibrateParams:\n",
    "    from random import seed as Seed, random as Random, randrange as RandIntRange\n",
    "\n",
    "    #Should usually be called before RecomputeFallowTS(), else the latter will use old parameters.\n",
    "    def UpdateParams(parameters):\n",
    "        global temporalZScoreThresholdMax\n",
    "        global temporalZScoreThresholdRange\n",
    "        global spatialMedianMultiplierMax\n",
    "        global spatialMedianMultiplierRange\n",
    "\n",
    "        if coupleParams:\n",
    "            temporalZScoreThresholdMax = temporalZScoreThresholdRange = parameters[0]\n",
    "            spatialMedianMultiplierMax = spatialMedianMultiplierRange =  parameters[1]\n",
    "        else:\n",
    "            temporalZScoreThresholdMax = parameters[0]\n",
    "            temporalZScoreThresholdRange = parameters[1]\n",
    "            spatialMedianMultiplierMax = parameters[2]\n",
    "            spatialMedianMultiplierRange = parameters[3]\n",
    "\n",
    "    def RecomputeFallowTS():\n",
    "        return ComputeFallowTS(temporalAnomalies.map(TemporalAnomalyAnalysis),\n",
    "                            spatialStats.map(SpatialAnomalyAnalysis))\n",
    "\n",
    "    #assumes fallowTS is up to date\n",
    "    def SampleIsFallowAtValidationPoints(fallowTS) -> ee.FeatureCollection:\n",
    "        isFallowRaster = fallowTS.select(f\"..{calibrationYear}\") #output of fallowTS generation prepends an int from 0 onwards then an underscore.\n",
    "        bandName = isFallowRaster.bandNames().get(0)\n",
    "\n",
    "        #mappable function\n",
    "        def Sample(point):\n",
    "            value = ee.Number(isFallowRaster.sample(region = point.geometry(), scale = targetOutputScale).first().get(bandName))\n",
    "            return point.set({\"estimate\" : value})\n",
    "\n",
    "        return testAreaSamples.map(Sample)\n",
    "        \n",
    "\n",
    "    #assumes fallowTS is up to date\n",
    "    def ObjectiveFunction(fallowTS):\n",
    "        data = SampleIsFallowAtValidationPoints(fallowTS)\n",
    "\n",
    "        #mappable function, to help with confusion matrix generation\n",
    "        def AddCompositeColumn(point):\n",
    "            return point.set({\"compositeScore\" : ee.Number((ee.Number(2).multiply(point.get(\"estimate\")).add(point.get(calibrationAttribName))))})\n",
    "\n",
    "        adjustedSamples = ee.Dictionary(data.map(AddCompositeColumn).aggregate_array(\"compositeScore\").reduce(ee.Reducer.frequencyHistogram()))\n",
    "        confArray = ee.Array([[adjustedSamples.get(\"3.0\", 0.0), adjustedSamples.get(\"1.0\", 0.0)], [adjustedSamples.get(\"2.0\", 0.0), adjustedSamples.get(\"0.0\", 0.0)]], ee.PixelType.float())\n",
    "        confMatrix = ee.ConfusionMatrix(confArray.long())\n",
    "        \n",
    "        return confMatrix.accuracy().getInfo()\n",
    "\n",
    "    def StopOptimisation(currentIteration, currentScore, lastScore) -> bool:\n",
    "        if currentIteration < minIterations:\n",
    "            return False\n",
    "\n",
    "        if currentIteration >= maxIterations:\n",
    "            print(f\"Stopping after reaching max iterations\")\n",
    "            return True\n",
    "\n",
    "        errorMetricChange = abs((lastScore - currentScore) / lastScore)\n",
    "        if errorMetricChange <= minChangeThreshold:\n",
    "            print(f\"Stopping for stagnating improvements. Change percentage = {errorMetricChange}\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def RandomParams(ranges: list[list]) -> list:\n",
    "        params = []\n",
    "        for range in ranges:\n",
    "            params.append(range[0] + Random() * (range[1] - range[0]))\n",
    "        return params\n",
    "\n",
    "    def RandomAgents(agents: list[list], excludeID: int, count = 3) -> list[list]:\n",
    "        picksIDs = []\n",
    "        \n",
    "        while len(picksIDs) < count:\n",
    "            i = RandIntRange(0, len(agents))\n",
    "            if i not in picksIDs and i != excludeID:\n",
    "                picksIDs.append(i)\n",
    "\n",
    "        return [agents[i] for i in picksIDs]\n",
    "\n",
    "    def Clamp(value: float, range: list[float, float]) -> float:\n",
    "        return max(min(value, max(range[0], range[1])), min(range[0], range[1]))\n",
    "\n",
    "    #init population and run vars\n",
    "    agents = [] #TODO consider switching agents and scores lists to a single priority queue (heapq). \n",
    "    scores = [] #same index as agents\n",
    "    bestScore = [0.0, []] #[score, [params giving this score]]\n",
    "    currentScore = 0.0\n",
    "    iteration = 0\n",
    "\n",
    "    Seed(diffEvSeed)\n",
    "\n",
    "    #check to avoid logical errors in the input paramRanges\n",
    "    if coupleParams and len(paramRanges) > 2:\n",
    "        print (f\"Warning! paramRanges supplied greater than 2.\\nAre you using ranges for decoupled params?\\nAttempting to use first and third ranges\")\n",
    "        paramRanges = [paramRanges[0], paramRanges[2]]\n",
    "        print (f\"Using ranges {paramRanges}\")\n",
    "\n",
    "    #while the theory says agents should be randomized, let's set the first two to something familiar (the defaults from the original research, and midpoints of the ranges)\n",
    "    agents += [[-3.0, -3.0, 0.8, 0.8] if not coupleParams else [-3.0, 0.8], #Wallace et al\n",
    "            [(paramRanges[i][0] + paramRanges[i][1]) / 2.0 for i in range(0, len(paramRanges))]]\n",
    "    for initalAgent in agents: #compute their scores\n",
    "        UpdateParams(initalAgent)\n",
    "        scores.append(ObjectiveFunction(RecomputeFallowTS()))\n",
    "\n",
    "    #generate the remaining agents (randomly)\n",
    "    for i in range (0, popSize - len(agents)): #the subtraction to avoid bugs in case I changed the initial values above.\n",
    "        randAgent = RandomParams(paramRanges)\n",
    "        while randAgent in agents: #Should be a practical impossibility to happen, but still...\n",
    "            randAgent = RandomParams(paramRanges)\n",
    "        agents.append(randAgent)\n",
    "        UpdateParams(randAgent)\n",
    "        scores.append(ObjectiveFunction(RecomputeFallowTS()))\n",
    "\n",
    "    #init bestScore from our randomely generated agents\n",
    "    for i in range(0,popSize):\n",
    "        if scores[i] > bestScore[0]:\n",
    "            bestScore = [scores[i], agents[i]]\n",
    "\n",
    "    print (f\"Agents = {len(agents)}\")\n",
    "    for i in range (0, popSize): print(f\"{scores[i]} -- {agents[i]}\")\n",
    "\n",
    "    print (\"==================================\\n\\t\\t\\t\\tStarting optimisation loop\\n==================================\")\n",
    "    while not StopOptimisation(iteration, currentScore, bestScore[0]):\n",
    "        print (f\"------------------------------\\nIteration {iteration} -- params: {bestScore[1]} -- score current: {bestScore[0]} last: {currentScore}\")\n",
    "        currentScore = bestScore[0]\n",
    "\n",
    "        for i in range (0, popSize):\n",
    "            assistVectors = RandomAgents(agents, i, 3)\n",
    "\n",
    "            testedAgent = list(agents[i])\n",
    "            rIndex = RandIntRange(0, len(paramRanges))\n",
    "            weights = RandomParams([[0.0, 1.0]] * len(paramRanges))\n",
    "            for j in range (0, len(paramRanges)):\n",
    "                if weights[j] < crossOverProb or j == rIndex:\n",
    "                    testedAgent[j] = Clamp(assistVectors[0][j] + diffWeight * (assistVectors[1][j] - assistVectors[2][j]), paramRanges[j])\n",
    "\n",
    "            UpdateParams(testedAgent)\n",
    "            testedScore = ObjectiveFunction(RecomputeFallowTS())\n",
    "\n",
    "            if (testedScore > scores[i]):\n",
    "                print (f\"\\t\\tAgent {i} improved from {scores[i]} to {testedScore} - Now: {testedAgent}\")\n",
    "                agents[i] = testedAgent\n",
    "                scores[i] = testedScore\n",
    "            else:\n",
    "                print (f\"\\t\\tAgent {i} no improvement ({scores[i]})\")\n",
    "            \n",
    "            if(testedScore > bestScore[0]):\n",
    "                bestScore = [testedScore, testedAgent]\n",
    "\n",
    "        print(f\"Best scorer: {bestScore[1]} -- {bestScore[0]}\")\n",
    "        iteration += 1\n",
    "        pass\n",
    "\n",
    "    print (f\"=================\\n Finished after {iteration} iterations\")\n",
    "    print (f\"{bestScore[0]} -- {bestScore[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result export (when not calibrating)\n",
    "\n",
    "if not autoCalibrateParams:\n",
    "    noDataValue = -9999\n",
    "\n",
    "    #split raster into multiple ones based on zone, then export\n",
    "    if exportSingleRaster: \n",
    "        exportName = f\"{projectName}_allZones_{dataset}_fallow-ts_{yearStart}-{yearEnd}_season_{targetMonths[0]}-{targetMonths[1]}-{targetMonths[2]}-{targetMonths[3]}\"\n",
    "        print (f\"exporting file: {exportName}\")\n",
    "        task = ee.batch.Export.image.toDrive(image = fallowTS.clip(roi.geometry()),\n",
    "                                                 description = exportName,\n",
    "                                                 region = roi.geometry(),\n",
    "                                                 scale = targetOutputScale,\n",
    "                                                 crs = 'EPSG:4326',\n",
    "                                                 maxPixels = 300000000,\n",
    "                                                 fileFormat = 'GeoTIFF',\n",
    "                                                 formatOptions = {'noData': noDataValue})\n",
    "\n",
    "        task.start()\n",
    "    else:\n",
    "        zones = roi.toList(roi.size())\n",
    "        zonesSize = zones.size().getInfo()\n",
    "\n",
    "        for i in range (0, zonesSize):\n",
    "            targetZone = ee.Feature(zones.get(i))\n",
    "            \n",
    "            exportName = f\"{projectName}_{targetZone.get(subdivisionPropertyName).getInfo()}_{dataset}_fallow-ts_{yearStart}-{yearEnd}_season_{targetMonths[0]}-{targetMonths[1]}-{targetMonths[2]}-{targetMonths[3]}\"\n",
    "            print (f\"{i} - exporting file: {exportName}\")\n",
    "            \n",
    "            task = ee.batch.Export.image.toDrive(image = fallowTS.clip(targetZone.geometry()),\n",
    "                                                 description = exportName,\n",
    "                                                 region = targetZone.geometry(),\n",
    "                                                 scale = targetOutputScale,\n",
    "                                                 crs = 'EPSG:4326',\n",
    "                                                 maxPixels = 300000000,\n",
    "                                                 fileFormat = 'GeoTIFF',\n",
    "                                                 formatOptions = {'noData': noDataValue},)\n",
    "                                                 #priority = i) can only be set for \"commercial projects\"\n",
    "\n",
    "            task.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
